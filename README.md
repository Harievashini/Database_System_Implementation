                                                                                  
                                                    
<p align= "right"> <b> Members:</b></p>
<p align= "right"> Harie Vashini </p>
<p align= "right"> Zeba Khan </p>

   <p align= "center"> <b> Database System Implementation</b></p>
   <p align= "center"> <b> Project Part I </b></p>
   <p align= "center"> <b>Data Generation & system Selection </b> </p>

<b>Description:</b>

This project involves evaluation of a single system, Postgres by changing system parameters and varying relation sizes. The data has been generated based on the Wisconsin Benchmark specifications. A Postgres instance has been created on GCloud SQL with name of the database as "wisconsin". A python code has been used to generate the csv data for wisconsin benchmark and is run using the command ("python3 benchmark.py"). ONEKTUP.csv, TENKTUP1.csv, TENKTUP2.csv are the three csv files that were generated using the python code.\copy command is used to export the csv files to the respective tables in the wisconsin database.

<b>Data Generator Code:</b> benchmark.py

<b>System :</b> PostgreSQL

This project uses PostgreSQL. PostgreSQL is a strong relational database as it provides many good features when compared to other databases and has an easy syntax. We both have a good working knowledge on PostgreSQL from CS586 Introduction to Database Management class. Postgres provides better language support for Python and flexibility for user defined data types for any complex computation to be foreseen. It supports many useful data types like enumerated types, network address types, geometric/spatial types, XML and JSON types, boolean. The GUI tool Pgadmin of PostgreSQl makes it easy to write and execute queries. PostgreSQL provides some advantages like there is no constraint on the size of the database. An individual table can be up to 32 terabytes with around 250 columns, even an individual field can hold upto a gigabyte of data. 

<b>Demonstration:</b>
The tables in the wisconsin database are:
ONEKTUP, TENKTUP1, TENKTUP2 with sizes 1000, 10000 and 10000. 

These were created using the following command:

*"create table ONEKTUP(unique1 INTEGER UNIQUE NOT NULL,unique2 INTEGER PRIMARY KEY,two INTEGER NOT NULL,four INTEGER NOT NULL,ten INTEGER NOT NULL,twenty INTEGER NOT NULL,onePercent INTEGER NOT NULL,tenPercent INTEGER NOT NULL,twentyPercent INTEGER NOT NULL,fiftyPercent INTEGER NOT NULL,unique3 INTEGER NOT NULL,evenOnePercent INTEGER NOT NULL,oddOnePercent INTEGER NOT NULL,stringu1 CHAR(52) UNIQUE NOT NULL,stringu2 CHAR(52) UNIQUE NOT NULL,string4 CHAR(52) NOT NULL);"*

After the tables have been created, the data has been populated into them using:

*"\copy ONEKTUP(unique1,unique2,two,four,ten,twenty,onePercent,tenPercent,twentyPercent,fiftyPercent,unique3,evenOnePercent,oddOnePercent,stringu1,stringu2,string4) FROM '/home/hariev2/DBI/ONEKTUP.csv' DELIMITER ',' CSV HEADER;"*

with the 3 csv files that were generated by the benchmark generator python code. 
The Generated csv files are placed in folder named ‘Generated Data’.

<b>Lessons learnt:</b>
- The Wisconsin paper provides a great detail into how different databases can be designed based on given specifications. 
- A python code to generate data is an efficient way to create and insert data from a csv file to SQL tables. This helped us to learn the faster way to perform this operation.
- It also helped in understanding the schema better for successful data population into the database.
- We learnt how to use Google Cloud Platform (GCLOUD SQL). 

<b>Challenges:</b> 
- Initial setup on the local system was difficult which then led us to use GCP for this project.
- Choosing the best language(Python, Java) for efficient generation of data into our database tables.

<b>References:</b> 
- https://github.com/snuchhi/DB-Benchmark-project/blob/master/benchmark-project.py
- Paper - The Wisconsin Benchmark: Past, Present, and Future
